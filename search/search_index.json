{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#llm-discord-bot","title":"LLM Discord Bot","text":"<p>A clean template to kickstart your deep learning project \ud83d\ude80\u26a1\ud83d\udd25 Click on Use this template to initialize new repository.</p> <p>Suggestions are always welcome!</p>"},{"location":"#description","title":"Description","text":"<p>This is a template for you to use for any project. It has all the actions set up for you to use.</p>"},{"location":"#for-more-info-check-the-docs","title":"For More info, check the Docs","text":""},{"location":"Reference/sdk/image/","title":"Image","text":""},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator","title":"ImageGenerator","text":"<p>               Bases: <code>Config</code></p> <p>Methods:</p> Name Description <code>gen_image</code>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.huggingface_api_token","title":"huggingface_api_token","text":"<pre><code>huggingface_api_token: str = Field(..., description='The api token from huggingface for calling models.', examples=['hf_zdZ...'], alias='HUGGINGFACE_API_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.xai_api_key","title":"xai_api_key","text":"<pre><code>xai_api_key: str = Field(..., description='The api key from x.ai for calling models.', examples=['xai-...'], alias='XAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.openai_api_key","title":"openai_api_key","text":"<pre><code>openai_api_key: str = Field(..., description='The api key from openai for calling models.', examples=['sk-proj-...'], alias='OPENAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.googleai_api_key","title":"googleai_api_key","text":"<pre><code>googleai_api_key: str = Field(..., description='The api key from googleai for calling models.', examples=['AIz...'], alias='GOOGLEAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.discord_bot_token","title":"discord_bot_token","text":"<pre><code>discord_bot_token: str = Field(..., description='The token from discord for calling models.', examples=['MTEz-...'], alias='DISCORD_BOT_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.gen_image","title":"gen_image","text":"<pre><code>gen_image(prompt: str) -&gt; bytes\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/image.py</code> <pre><code>async def gen_image(self, prompt: str) -&gt; bytes:\n    async with httpx.AsyncClient(\n        base_url=\"https://api-inference.huggingface.co/models\"\n    ) as client:\n        response = await client.post(\n            url=\"/strangerzonehf/Flux-Animex-v2-LoRA\",\n            headers={\"Authorization\": f\"Bearer {self.huggingface_api_token}\"},\n            json={\"inputs\": prompt},\n            timeout=60,\n        )\n        if response.status_code != 200:\n            raise Exception(\n                f\"Failed to generate image: {response.status_code} {response.text}\"\n            )\n        return response.content\n</code></pre>"},{"location":"Reference/sdk/image/#src.sdk.image.ImageGenerator.gen_image(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/","title":"Llm","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices","title":"LLMServices","text":"<p>               Bases: <code>Config</code></p> <p>Methods:</p> Name Description <code>get_xai_reply</code> <code>get_xai_reply_stream</code> <code>get_oai_reply</code> <code>get_oai_reply_stream</code> <code>get_gai_reply</code> <code>get_gai_reply_stream</code>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.huggingface_api_token","title":"huggingface_api_token","text":"<pre><code>huggingface_api_token: str = Field(..., description='The api token from huggingface for calling models.', examples=['hf_zdZ...'], alias='HUGGINGFACE_API_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.xai_api_key","title":"xai_api_key","text":"<pre><code>xai_api_key: str = Field(..., description='The api key from x.ai for calling models.', examples=['xai-...'], alias='XAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.openai_api_key","title":"openai_api_key","text":"<pre><code>openai_api_key: str = Field(..., description='The api key from openai for calling models.', examples=['sk-proj-...'], alias='OPENAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.googleai_api_key","title":"googleai_api_key","text":"<pre><code>googleai_api_key: str = Field(..., description='The api key from googleai for calling models.', examples=['AIz...'], alias='GOOGLEAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.discord_bot_token","title":"discord_bot_token","text":"<pre><code>discord_bot_token: str = Field(..., description='The token from discord for calling models.', examples=['MTEz-...'], alias='DISCORD_BOT_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply","title":"get_xai_reply","text":"<pre><code>get_xai_reply(prompt: str) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_xai_reply(self, prompt: str) -&gt; ChatCompletion:\n    client = AsyncOpenAI(api_key=self.xai_api_key, base_url=\"https://api.x.ai/v1\")\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"grok-beta\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply_stream","title":"get_xai_reply_stream","text":"<pre><code>get_xai_reply_stream(prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_xai_reply_stream(self, prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(api_key=self.xai_api_key, base_url=\"https://api.x.ai/v1\")\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"grok-beta\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply","title":"get_oai_reply","text":"<pre><code>get_oai_reply(prompt: str) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_oai_reply(self, prompt: str) -&gt; ChatCompletion:\n    client = AsyncOpenAI(api_key=self.openai_api_key, base_url=\"https://api.openai.com/v1\")\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply_stream","title":"get_oai_reply_stream","text":"<pre><code>get_oai_reply_stream(prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_oai_reply_stream(self, prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(api_key=self.openai_api_key, base_url=\"https://api.openai.com/v1\")\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply","title":"get_gai_reply","text":"<pre><code>get_gai_reply(prompt: str) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_gai_reply(self, prompt: str) -&gt; ChatCompletion:\n    client = AsyncOpenAI(\n        api_key=self.googleai_api_key,\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    )\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"gemini-1.5-pro\",\n        n=1,\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply_stream","title":"get_gai_reply_stream","text":"<pre><code>get_gai_reply_stream(prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_gai_reply_stream(self, prompt: str) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(\n        api_key=self.googleai_api_key,\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    )\n    await self._get_models(client=client)\n\n    completion = client.chat.completions.create(\n        model=\"gemini-1.5-pro\",\n        n=1,\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\"role\": \"user\", \"content\": prompt},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/types/config/","title":"Config","text":""},{"location":"Reference/types/config/#src.types.config.Config","title":"Config","text":"<p>               Bases: <code>BaseSettings</code></p>"},{"location":"Reference/types/config/#src.types.config.Config.huggingface_api_token","title":"huggingface_api_token","text":"<pre><code>huggingface_api_token: str = Field(..., description='The api token from huggingface for calling models.', examples=['hf_zdZ...'], alias='HUGGINGFACE_API_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.xai_api_key","title":"xai_api_key","text":"<pre><code>xai_api_key: str = Field(..., description='The api key from x.ai for calling models.', examples=['xai-...'], alias='XAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.openai_api_key","title":"openai_api_key","text":"<pre><code>openai_api_key: str = Field(..., description='The api key from openai for calling models.', examples=['sk-proj-...'], alias='OPENAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.googleai_api_key","title":"googleai_api_key","text":"<pre><code>googleai_api_key: str = Field(..., description='The api key from googleai for calling models.', examples=['AIz...'], alias='GOOGLEAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.discord_bot_token","title":"discord_bot_token","text":"<pre><code>discord_bot_token: str = Field(..., description='The token from discord for calling models.', examples=['MTEz-...'], alias='DISCORD_BOT_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Scripts/gen_docs/","title":"Gen docs","text":""},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator","title":"DocsGenerator","text":"<p>               Bases: <code>BaseModel</code></p> <p>DocsGenerator is a class that generates documentation for Python files or classes within a specified source directory.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source directory or file path.</p> <code>output</code> <code>str</code> <p>The output directory path.</p> <code>exclude</code> <code>str</code> <p>Comma-separated list of folders or files to exclude.</p> <code>mode</code> <code>Literal['file', 'class']</code> <p>Mode of documentation generation, either by file or class.</p> <p>Methods:</p> Name Description <code>gen_docs</code> <p>Generates documentation by file or class.</p> <code>__call__</code> <p>Asynchronously calls the gen_docs method.</p> Using CLI <pre><code>python ./scripts/gen_docs.py --source ./src --output ./docs/Reference --exclude .venv gen_docs\n</code></pre> Using Rye <pre><code>rye run gen\n</code></pre> <p>Methods:</p> Name Description <code>gen_docs</code> <p>This function can generate docs by file or class.</p>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.source","title":"source","text":"<pre><code>source: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.output","title":"output","text":"<pre><code>output: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.exclude","title":"exclude","text":"<pre><code>exclude: str = Field(default='.venv', description='Exclude the folder or file, it should be separated by comma.', examples=['.venv,.git,.idea'])\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.mode","title":"mode","text":"<pre><code>mode: Literal['file', 'class'] = Field(default='class', description='Generate docs by file or class.')\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.gen_docs","title":"gen_docs","text":"<pre><code>gen_docs() -&gt; None\n</code></pre> <p>This function can generate docs by file or class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source path is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n&gt;&gt;&gt; for key, value in pair_list.items():\n...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n...     asyncio.run(docs_generator.gen_docs())\n</code></pre> Source code in <code>scripts/gen_docs.py</code> <pre><code>async def gen_docs(self) -&gt; None:\n    \"\"\"This function can generate docs by file or class.\n\n    Raises:\n        ValueError: If the source path is invalid.\n\n    Examples:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n        &gt;&gt;&gt; for key, value in pair_list.items():\n        ...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n        ...     asyncio.run(docs_generator.gen_docs())\n    \"\"\"\n    with Progress() as progress:\n        task = progress.add_task(\"[green]Generating docs...\")\n        if self._source_path.is_dir():\n            await self.__remove_existing_folder()\n\n            need_to_exclude = [*self.exclude.split(\",\"), \"__init__.py\"]\n            files = self._source_path.glob(\"**/*.py\")\n            all_files = [\n                file for file in files if not any(f in file.parts for f in need_to_exclude)\n            ]\n\n            progress.update(\n                task_id=task, description=\"[cyan]Files Found...\", total=len(all_files)\n            )\n\n            for file in all_files:\n                docs_path = Path(\n                    f\"{self._output_path}/{file.parent.relative_to(self._source_path)}\"\n                )\n                processed_file = await self.__gen_single_docs(docs_path=docs_path, file=file)\n                progress.update(\n                    task_id=task,\n                    advance=1,\n                    description=f\"[cyan]Processing {processed_file}...\",\n                    refresh=True,\n                )\n\n        elif self._source_path.is_file():\n            progress.update(task_id=task, description=\"[cyan]Files Found...\", total=1)\n            processed_file = await self.__gen_single_docs(self._output_path, self._source_path)\n            progress.update(\n                task_id=task,\n                advance=1,\n                description=f\"[cyan]Processing {processed_file}...\",\n                refresh=True,\n            )\n        else:\n            raise ValueError(\"Invalid source path\")\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/10/06/this-is-an-example-post-for-blog/","title":"This is an example post for blog","text":"<p>This is an simple example post for blog.</p>"},{"location":"installation/pip/","title":"Using PIP","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/llm_discord_bot\nmv template your-repo-name\n\n# change directory\ncd your-repo-name\n\n# [OPTIONAL] create conda environment\nconda create -n myenv python=3.9\nconda activate myenv\n\n# install requirements\npip install -r requirements.lock\n</code></pre>"},{"location":"installation/rye/","title":"Using Rye","text":""},{"location":"installation/rye/#step-1-install-rye","title":"Step 1: Install Rye","text":"<ul> <li>Visit the Rye Installation for installation.</li> </ul>"},{"location":"installation/rye/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/llm_discord_bot\nmv template your-repo-name\n\n# change directory\ncd your-repo-name\n</code></pre>"},{"location":"installation/rye/#step-3-install-requirements","title":"Step 3: Install requirements","text":"<pre><code>rye sync\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}