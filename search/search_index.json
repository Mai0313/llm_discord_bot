{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#llm-discord-bot","title":"LLM Discord Bot","text":"<p>A clean template to kickstart your deep learning project \ud83d\ude80\u26a1\ud83d\udd25 Click on Use this template to initialize new repository.</p> <p>Suggestions are always welcome!</p>"},{"location":"#description","title":"Description","text":"<p>This is a template for you to use for any project. It has all the actions set up for you to use.</p>"},{"location":"#for-more-info-check-the-docs","title":"For More info, check the Docs","text":""},{"location":"Reference/cogs/gen_image/","title":"Gen image","text":""},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs","title":"ImageGeneratorCogs","text":"<pre><code>ImageGeneratorCogs(bot: commands.Bot)\n</code></pre> <p>               Bases: <code>Cog</code></p> <p>Parameters:</p> Name Type Description Default <code>Bot</code> required <p>Methods:</p> Name Description <code>gen_image</code> <code>gen</code> <p>\u751f\u6210\u5716\u7247\u4e26\u6a19\u8a18\u767c\u9001\u8a0a\u606f\u7684\u4eba\u3002</p> Source code in <code>src/cogs/gen_image.py</code> <pre><code>def __init__(self, bot: commands.Bot):\n    self.bot = bot\n    self.config = Config()\n</code></pre>"},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs(bot)","title":"<code>bot</code>","text":""},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.bot","title":"bot","text":"<pre><code>bot = bot\n</code></pre>"},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.config","title":"config","text":"<pre><code>config = Config()\n</code></pre>"},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.gen_image","title":"gen_image","text":"<pre><code>gen_image(prompt: str) -&gt; bytes\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required Source code in <code>src/cogs/gen_image.py</code> <pre><code>async def gen_image(self, prompt: str) -&gt; bytes:\n    async with httpx.AsyncClient(\n        base_url=\"https://api-inference.huggingface.co/models\"\n    ) as client:\n        response = await client.post(\n            url=\"/strangerzonehf/Flux-Animex-v2-LoRA\",\n            headers={\"Authorization\": f\"Bearer {self.config.huggingface_api_token}\"},\n            json={\"inputs\": prompt},\n            timeout=60,\n        )\n        if response.status_code != 200:\n            raise Exception(\n                f\"Failed to generate image: {response.status_code} {response.text}\"\n            )\n        return response.content\n</code></pre>"},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.gen_image(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.gen","title":"gen","text":"<pre><code>gen(ctx: commands.Context, *, prompt: str) -&gt; None\n</code></pre> <p>\u751f\u6210\u5716\u7247\u4e26\u6a19\u8a18\u767c\u9001\u8a0a\u606f\u7684\u4eba\u3002</p> <p>Parameters:</p> Name Type Description Default <code>Context</code> required <code>str</code> required Source code in <code>src/cogs/gen_image.py</code> <pre><code>@commands.command()\nasync def gen(self, ctx: commands.Context, *, prompt: str) -&gt; None:\n    \"\"\"\u751f\u6210\u5716\u7247\u4e26\u6a19\u8a18\u767c\u9001\u8a0a\u606f\u7684\u4eba\u3002\"\"\"\n    msg = await ctx.send(f\"{ctx.author.mention} \u5716\u7247\u6b63\u5728\u751f\u6210\u4e2d...\\n\u9032\u5ea6: [----------] 0%\")\n    total_steps = 5  # \u6a21\u64ec\u7684\u9032\u5ea6\u968e\u6bb5\u6578\n\n    # \u6a21\u64ec\u9032\u5ea6\u689d\u66f4\u65b0\n    for step in range(1, total_steps + 1):\n        progress = int((step / total_steps) * 100)\n        progress_bar = (\n            f\"\u9032\u5ea6: [{'\u2588' * (progress // 10)}{'-' * (10 - progress // 10)}] {progress}%\"\n        )\n        await msg.edit(content=f\"\u5716\u7247\u6b63\u5728\u751f\u6210\u4e2d...\\n{progress_bar}\")\n        await asyncio.sleep(5)  # \u6bcf5\u79d2\u66f4\u65b0\u4e00\u6b21\n\n    # \u8acb\u6c42 Hugging Face API \u751f\u6210\u5716\u7247\n    try:\n        image_bytes = await self.gen_image(prompt=prompt)\n        bytes_io = BytesIO(image_bytes)\n        image = Image.open(bytes_io)\n\n        # \u5c07\u5716\u7247\u5b58\u70ba\u66ab\u6642\u6a94\u6848\u4e26\u767c\u9001\n        with BytesIO() as image_binary:\n            image.save(image_binary, format=\"PNG\")\n            image_binary.seek(0)\n\n            # \u7de8\u8f2f\u5b8c\u6210\u8a0a\u606f\u4e26\u767c\u9001\u5716\u7247\n            await msg.edit(content=f\"{ctx.author.mention} \u5716\u7247\u751f\u6210\u5b8c\u6210\\nPrompt: `{prompt}`\")\n            await ctx.send(file=discord.File(fp=image_binary, filename=\"generated_image.png\"))\n    except Exception as e:\n        await msg.edit(content=f\"{ctx.author.mention} \u5716\u7247\u751f\u6210\u5931\u6557\\n\u932f\u8aa4: {e!s}\")\n</code></pre>"},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.gen(ctx)","title":"<code>ctx</code>","text":""},{"location":"Reference/cogs/gen_image/#src.cogs.gen_image.ImageGeneratorCogs.gen(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/cogs/log_message/","title":"Log message","text":""},{"location":"Reference/cogs/log_message/#src.cogs.log_message.LogMessageCogs","title":"LogMessageCogs","text":"<pre><code>LogMessageCogs(bot: commands.Bot)\n</code></pre> <p>               Bases: <code>Cog</code></p> <p>Parameters:</p> Name Type Description Default <code>Bot</code> required <p>Methods:</p> Name Description <code>on_message</code> Source code in <code>src/cogs/log_message.py</code> <pre><code>def __init__(self, bot: commands.Bot):\n    self.bot = bot\n</code></pre>"},{"location":"Reference/cogs/log_message/#src.cogs.log_message.LogMessageCogs(bot)","title":"<code>bot</code>","text":""},{"location":"Reference/cogs/log_message/#src.cogs.log_message.LogMessageCogs.bot","title":"bot","text":"<pre><code>bot = bot\n</code></pre>"},{"location":"Reference/cogs/log_message/#src.cogs.log_message.LogMessageCogs.on_message","title":"on_message","text":"<pre><code>on_message(message: discord.Message) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>Message</code> required Source code in <code>src/cogs/log_message.py</code> <pre><code>@commands.Cog.listener()\nasync def on_message(self, message: discord.Message) -&gt; None:\n    # \u5ffd\u7565\u6a5f\u5668\u4eba\u81ea\u5df1\u7684\u8a0a\u606f\n    if message.author.bot:\n        return\n\n    # \u8a18\u9304\u8a0a\u606f\n    \"\"\"\u5c07\u8a0a\u606f\u8a18\u9304\u5230\u6a94\u6848\uff0c\u5305\u542b\u9644\u4ef6\u548c\u8cbc\u5716\u4e0b\u8f09\"\"\"\n    # \u751f\u6210\u4fdd\u5b58\u8def\u5f91\uff08\u4f9d\u64da\u65e5\u671f\uff09\n    today = datetime.date.today().isoformat()\n\n    if isinstance(message.channel, discord.DMChannel):\n        # \u5982\u679c\u662f\u79c1\u8a0a\uff0c\u4f7f\u7528\u7528\u6236 ID \u4f5c\u70ba\u540d\u7a31\n        channel_name = f\"DM_{message.author.id}\"\n    else:\n        # \u5982\u679c\u662f\u4f3a\u670d\u5668\u4e2d\u7684\u983b\u9053\uff0c\u4f7f\u7528\u983b\u9053\u540d\u7a31\n        channel_name = f\"{message.channel.name}_{message.channel.id}\"\n\n    save_dir = Path(\"logs\") / today / channel_name\n\n    # \u78ba\u4fdd\u8cc7\u6599\u593e\u5b58\u5728\n    save_dir.mkdir(parents=True, exist_ok=True)\n\n    # \u5efa\u7acb\u65e5\u8a8c\u6a94\u6848\u540d\u7a31\n    log_file = save_dir / \"log.txt\"\n\n    # \u5224\u65b7\u662f\u79c1\u8a0a\u9084\u662f\u4f3a\u670d\u5668\u983b\u9053\n    if isinstance(message.channel, discord.DMChannel):\n        channel_info = f\"DM_{message.author.id}\"\n    else:\n        channel_info = f\"{message.channel.name} ({message.channel.id})\"\n\n    # \u8a18\u9304\u8a0a\u606f\u5167\u5bb9\n    message_info = f\"{message.author} ({message.author.id}) at {message.created_at.strftime('%Y-%m-%d %H:%M:%S')} in {channel_info}:\\n\"\n    message_content = f\"{message.content}\\n\"\n    log_entry = f\"{message_info}{message_content}{'-' * 40}\\n\"\n\n    # \u4f7f\u7528 anyio \u975e\u540c\u6b65\u5beb\u5165\n    async with await anyio.open_file(log_file, mode=\"a\", encoding=\"utf-8\") as f:\n        await f.write(log_entry)\n\n    logfire.info(\n        f\"{message.author.name}: {message.content}\",\n        author_id=message.author.id,\n        created_time=message.created_at.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        channel_name=getattr(message.channel, \"name\", \"DM\"),\n        channel_id=message.channel.id,\n    )\n\n    # \u8655\u7406\u9644\u4ef6\n    for attachment in message.attachments:\n        attachment_path = save_dir / attachment.filename\n        await attachment.save(attachment_path)\n\n    # \u8655\u7406\u8cbc\u5716\n    if message.stickers:\n        for sticker in message.stickers:\n            sticker_path = save_dir / f\"sticker_{sticker.id}.png\"\n            await sticker.save(sticker_path)\n\n    # \u7e7c\u7e8c\u8655\u7406\u5176\u4ed6\u547d\u4ee4\n    await self.bot.process_commands(message)\n</code></pre>"},{"location":"Reference/cogs/log_message/#src.cogs.log_message.LogMessageCogs.on_message(message)","title":"<code>message</code>","text":""},{"location":"Reference/cogs/template/","title":"Template","text":""},{"location":"Reference/cogs/template/#src.cogs.template.TemplateCogs","title":"TemplateCogs","text":"<pre><code>TemplateCogs(bot: commands.Bot)\n</code></pre> <p>               Bases: <code>Cog</code></p> <p>Parameters:</p> Name Type Description Default <code>Bot</code> required <p>Methods:</p> Name Description <code>on_message</code> Source code in <code>src/cogs/template.py</code> <pre><code>def __init__(self, bot: commands.Bot):\n    self.bot = bot\n</code></pre>"},{"location":"Reference/cogs/template/#src.cogs.template.TemplateCogs(bot)","title":"<code>bot</code>","text":""},{"location":"Reference/cogs/template/#src.cogs.template.TemplateCogs.bot","title":"bot","text":"<pre><code>bot = bot\n</code></pre>"},{"location":"Reference/cogs/template/#src.cogs.template.TemplateCogs.on_message","title":"on_message","text":"<pre><code>on_message(message: discord.Message) -&gt; None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>Message</code> required Source code in <code>src/cogs/template.py</code> <pre><code>@commands.Cog.listener()\nasync def on_message(self, message: discord.Message) -&gt; None:\n    # \u5ffd\u7565\u4f86\u81ea\u6a5f\u5668\u4eba\u7684\u8a0a\u606f\n    if message.author.bot:\n        return\n\n    # \u5982\u679c\u8a0a\u606f\u5167\u5bb9\u662f \"DEBUG\"\uff0c\u5c0d\u8a72\u8a0a\u606f\u6309\u8b9a\n    if message.content.lower() == \"DEBUG\":\n        await message.add_reaction(\"\ud83d\udc4d\")\n</code></pre>"},{"location":"Reference/cogs/template/#src.cogs.template.TemplateCogs.on_message(message)","title":"<code>message</code>","text":""},{"location":"Reference/sdk/llm/","title":"Llm","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices","title":"LLMServices","text":"<p>               Bases: <code>Config</code></p> <p>Methods:</p> Name Description <code>get_xai_reply</code> <code>get_oai_reply</code> <code>get_gai_reply</code> <code>get_xai_reply_stream</code> <code>get_oai_reply_stream</code> <code>get_gai_reply_stream</code>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.huggingface_api_token","title":"huggingface_api_token","text":"<pre><code>huggingface_api_token: str = Field(..., description='The api token from huggingface for calling models.', examples=['hf_zdZ...'], alias='HUGGINGFACE_API_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.xai_api_key","title":"xai_api_key","text":"<pre><code>xai_api_key: str = Field(..., description='The api key from x.ai for calling models.', examples=['xai-...'], alias='XAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.openai_api_key","title":"openai_api_key","text":"<pre><code>openai_api_key: str = Field(..., description='The api key from openai for calling models.', examples=['sk-proj-...'], alias='OPENAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.googleai_api_key","title":"googleai_api_key","text":"<pre><code>googleai_api_key: str = Field(..., description='The api key from googleai for calling models.', examples=['AIz...'], alias='GOOGLEAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.discord_bot_token","title":"discord_bot_token","text":"<pre><code>discord_bot_token: str = Field(..., description='The token from discord for calling models.', examples=['MTEz-...'], alias='DISCORD_BOT_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.system_prompt","title":"system_prompt","text":"<pre><code>system_prompt: str = Field(default=SYSTEM_PROMPT)\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply","title":"get_xai_reply","text":"<pre><code>get_xai_reply(prompt: str, image: Optional[str] = None) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_xai_reply(self, prompt: str, image: Optional[str] = None) -&gt; ChatCompletion:\n    client = AsyncOpenAI(api_key=self.xai_api_key, base_url=\"https://api.x.ai/v1\")\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"grok-beta\",\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply(image)","title":"<code>image</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply","title":"get_oai_reply","text":"<pre><code>get_oai_reply(prompt: str, image: Optional[str] = None) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_oai_reply(self, prompt: str, image: Optional[str] = None) -&gt; ChatCompletion:\n    client = AsyncOpenAI(api_key=self.openai_api_key, base_url=\"https://api.openai.com/v1\")\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply(image)","title":"<code>image</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply","title":"get_gai_reply","text":"<pre><code>get_gai_reply(prompt: str, image: Optional[str] = None) -&gt; ChatCompletion\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_gai_reply(self, prompt: str, image: Optional[str] = None) -&gt; ChatCompletion:\n    client = AsyncOpenAI(\n        api_key=self.googleai_api_key,\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    )\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"gemini-1.5-pro\",\n        n=1,\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n    )\n    return await completion\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply(image)","title":"<code>image</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply_stream","title":"get_xai_reply_stream","text":"<pre><code>get_xai_reply_stream(prompt: str, image: Optional[str] = None) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_xai_reply_stream(\n    self, prompt: str, image: Optional[str] = None\n) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(api_key=self.xai_api_key, base_url=\"https://api.x.ai/v1\")\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"grok-beta\",\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_xai_reply_stream(image)","title":"<code>image</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply_stream","title":"get_oai_reply_stream","text":"<pre><code>get_oai_reply_stream(prompt: str, image: Optional[str] = None) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_oai_reply_stream(\n    self, prompt: str, image: Optional[str] = None\n) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(api_key=self.openai_api_key, base_url=\"https://api.openai.com/v1\")\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_oai_reply_stream(image)","title":"<code>image</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply_stream","title":"get_gai_reply_stream","text":"<pre><code>get_gai_reply_stream(prompt: str, image: Optional[str] = None) -&gt; AsyncGenerator[ChatCompletionChunk, None]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>str</code> required <code>Optional[str]</code> <code>None</code> Source code in <code>src/sdk/llm.py</code> <pre><code>async def get_gai_reply_stream(\n    self, prompt: str, image: Optional[str] = None\n) -&gt; AsyncGenerator[ChatCompletionChunk, None]:\n    client = OpenAI(\n        api_key=self.googleai_api_key,\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    )\n    await self._get_models(client=client)\n\n    content: list[dict[str, Any]] = [{\"type\": \"text\", \"text\": prompt}]\n    if image:\n        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"{image}\"}})\n\n    completion = client.chat.completions.create(\n        model=\"gemini-1.5-pro\",\n        n=1,\n        messages=[\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": content},\n        ],\n        stream=True,\n    )\n    for chunk in completion:\n        if len(chunk.choices) &gt; 0:\n            yield chunk\n</code></pre>"},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply_stream(prompt)","title":"<code>prompt</code>","text":""},{"location":"Reference/sdk/llm/#src.sdk.llm.LLMServices.get_gai_reply_stream(image)","title":"<code>image</code>","text":""},{"location":"Reference/types/config/","title":"Config","text":""},{"location":"Reference/types/config/#src.types.config.Config","title":"Config","text":"<p>               Bases: <code>BaseSettings</code></p>"},{"location":"Reference/types/config/#src.types.config.Config.huggingface_api_token","title":"huggingface_api_token","text":"<pre><code>huggingface_api_token: str = Field(..., description='The api token from huggingface for calling models.', examples=['hf_zdZ...'], alias='HUGGINGFACE_API_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.xai_api_key","title":"xai_api_key","text":"<pre><code>xai_api_key: str = Field(..., description='The api key from x.ai for calling models.', examples=['xai-...'], alias='XAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.openai_api_key","title":"openai_api_key","text":"<pre><code>openai_api_key: str = Field(..., description='The api key from openai for calling models.', examples=['sk-proj-...'], alias='OPENAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.googleai_api_key","title":"googleai_api_key","text":"<pre><code>googleai_api_key: str = Field(..., description='The api key from googleai for calling models.', examples=['AIz...'], alias='GOOGLEAI_API_KEY', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/types/config/#src.types.config.Config.discord_bot_token","title":"discord_bot_token","text":"<pre><code>discord_bot_token: str = Field(..., description='The token from discord for calling models.', examples=['MTEz-...'], alias='DISCORD_BOT_TOKEN', frozen=False, deprecated=False)\n</code></pre>"},{"location":"Reference/utils/attach/","title":"Attach","text":""},{"location":"Reference/utils/attach/#src.utils.attach.Attachment","title":"Attachment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> Name Description <code>get_data_url</code>"},{"location":"Reference/utils/attach/#src.utils.attach.Attachment.file_path","title":"file_path","text":"<pre><code>file_path: str = Field(..., description='The path to the file to be converted to a data URL')\n</code></pre>"},{"location":"Reference/utils/attach/#src.utils.attach.Attachment.get_data_url","title":"get_data_url","text":"<pre><code>get_data_url() -&gt; str\n</code></pre> Source code in <code>src/utils/attach.py</code> <pre><code>def get_data_url(self) -&gt; str:\n    # Guess the MIME type of the image based on the file extension\n    data_path = Path(self.file_path)\n    mime_type, _ = guess_type(data_path)\n    if mime_type is None:\n        mime_type = \"application/octet-stream\"  # Default MIME type if none is found\n\n    file_content = data_path.read_bytes()\n    base64_encoded_data = base64.b64encode(file_content).decode(\"utf-8\")\n\n    # Construct the data URL\n    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n</code></pre>"},{"location":"Scripts/gen_docs/","title":"Gen docs","text":""},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator","title":"DocsGenerator","text":"<p>               Bases: <code>BaseModel</code></p> <p>DocsGenerator is a class that generates documentation for Python files or classes within a specified source directory.</p> <p>Attributes:</p> Name Type Description <code>source</code> <code>str</code> <p>The source directory or file path.</p> <code>output</code> <code>str</code> <p>The output directory path.</p> <code>exclude</code> <code>str</code> <p>Comma-separated list of folders or files to exclude.</p> <code>mode</code> <code>Literal['file', 'class']</code> <p>Mode of documentation generation, either by file or class.</p> <p>Methods:</p> Name Description <code>gen_docs</code> <p>Generates documentation by file or class.</p> <code>__call__</code> <p>Asynchronously calls the gen_docs method.</p> Using CLI <pre><code>python ./scripts/gen_docs.py --source ./src --output ./docs/Reference --exclude .venv gen_docs\n</code></pre> Using Rye <pre><code>rye run gen\n</code></pre> <p>Methods:</p> Name Description <code>gen_docs</code> <p>This function can generate docs by file or class.</p>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.source","title":"source","text":"<pre><code>source: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.output","title":"output","text":"<pre><code>output: str = Field(..., frozen=True)\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.exclude","title":"exclude","text":"<pre><code>exclude: str = Field(default='.venv', description='Exclude the folder or file, it should be separated by comma.', examples=['.venv,.git,.idea'])\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.mode","title":"mode","text":"<pre><code>mode: Literal['file', 'class'] = Field(default='class', description='Generate docs by file or class.')\n</code></pre>"},{"location":"Scripts/gen_docs/#scripts.gen_docs.DocsGenerator.gen_docs","title":"gen_docs","text":"<pre><code>gen_docs() -&gt; None\n</code></pre> <p>This function can generate docs by file or class.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the source path is invalid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import asyncio\n&gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n&gt;&gt;&gt; for key, value in pair_list.items():\n...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n...     asyncio.run(docs_generator.gen_docs())\n</code></pre> Source code in <code>scripts/gen_docs.py</code> <pre><code>async def gen_docs(self) -&gt; None:\n    \"\"\"This function can generate docs by file or class.\n\n    Raises:\n        ValueError: If the source path is invalid.\n\n    Examples:\n        &gt;&gt;&gt; import asyncio\n        &gt;&gt;&gt; pair_list = {\"./src\": \"./docs/Reference\"}\n        &gt;&gt;&gt; for key, value in pair_list.items():\n        ...     docs_generator = DocsGenerator(source=key, output=value, exclude=\".venv\", mode=\"class\")\n        ...     asyncio.run(docs_generator.gen_docs())\n    \"\"\"\n    with Progress() as progress:\n        task = progress.add_task(\"[green]Generating docs...\")\n        if self._source_path.is_dir():\n            await self.__remove_existing_folder()\n\n            need_to_exclude = [*self.exclude.split(\",\"), \"__init__.py\"]\n            files = self._source_path.glob(\"**/*.py\")\n            all_files = [\n                file for file in files if not any(f in file.parts for f in need_to_exclude)\n            ]\n\n            progress.update(\n                task_id=task, description=\"[cyan]Files Found...\", total=len(all_files)\n            )\n\n            for file in all_files:\n                docs_path = Path(\n                    f\"{self._output_path}/{file.parent.relative_to(self._source_path)}\"\n                )\n                processed_file = await self.__gen_single_docs(docs_path=docs_path, file=file)\n                progress.update(\n                    task_id=task,\n                    advance=1,\n                    description=f\"[cyan]Processing {processed_file}...\",\n                    refresh=True,\n                )\n\n        elif self._source_path.is_file():\n            progress.update(task_id=task, description=\"[cyan]Files Found...\", total=1)\n            processed_file = await self.__gen_single_docs(self._output_path, self._source_path)\n            progress.update(\n                task_id=task,\n                advance=1,\n                description=f\"[cyan]Processing {processed_file}...\",\n                refresh=True,\n            )\n        else:\n            raise ValueError(\"Invalid source path\")\n</code></pre>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/10/06/this-is-an-example-post-for-blog/","title":"This is an example post for blog","text":"<p>This is an simple example post for blog.</p>"},{"location":"installation/pip/","title":"Using PIP","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/llm_discord_bot\nmv template your-repo-name\n\n# change directory\ncd your-repo-name\n\n# [OPTIONAL] create conda environment\nconda create -n myenv python=3.9\nconda activate myenv\n\n# install requirements\npip install -r requirements.lock\n</code></pre>"},{"location":"installation/rye/","title":"Using Rye","text":""},{"location":"installation/rye/#step-1-install-rye","title":"Step 1: Install Rye","text":"<ul> <li>Visit the Rye Installation for installation.</li> </ul>"},{"location":"installation/rye/#step-2-clone-the-repository","title":"Step 2: Clone the repository","text":"<pre><code># clone project\ngit clone https://github.com/Mai0313/llm_discord_bot\nmv template your-repo-name\n\n# change directory\ncd your-repo-name\n</code></pre>"},{"location":"installation/rye/#step-3-install-requirements","title":"Step 3: Install requirements","text":"<pre><code>rye sync\n</code></pre>"},{"location":"blog/archive/2024/","title":"2024","text":""}]}